
\chapter{Agrupamiento sobre Redes} % Agrupamiento en redes 
\label{chapter:2}
%\cleanchapterquote{A picture is worth a thousand words. An interface is worth a thousand pictures.}{Ben Shneiderman}{(Professor for Computer Science)}

%\begin{lstlisting}[language=Java, caption={A simple Hellow World example in %Java.}\label{lst:javahelloworld}]
%public class HelloWorld {
%	public static void main ( String[] args ) {
%		// Output Hello World!
%		System.out.println( "Hello World!" );
%	}
%}
%\end{lstlisting}

En este capítulo se presenta el problema principal y algunos elementos necesarios para comprender tanto su complejidad del mismo como una posible solución. Comenzamos con una definición formal de un red  y de su representación matemática; posteriormente, se presenta el problema de agrupamiento y su relevancia dentro del aprendizaje automático. Finalmente, el capítulo se enfoca en analizar la tarea de agrupamiento en una colección de redes, describiendo brevemente enfoques y limitaciones para resolver el problema. 

\section{Redes}

Una red es un conjunto de nodos unidos por aristas que representan relaciones. Los nodos y aristas los podemos encontrar en distintas disciplinas con distintos nombres, por ejemplo en física se denominan sitios y vínculos y en sociología actores y vínculos. 

La representación matemática de una red se denomina grafo y es estudiada en matemáticas discretas, específicamente en teoría de grafos. Un grafo está formalmente definido como un par de conjuntos $G = (V,E)$, donde $V$ es un conjunto no vacío de nodos (vértices) y $E$ es un conjunto de aristas (edges) \cite{saoub_graph_2021}.

 \begin{figure}[htbp]
   \centering
   \includesvg[width=0.25\textwidth]{figures/graph.svg}
    \caption{Grafo no dirigido de tres nodos y tres aristas.}
    \label{fig:graph}
\end{figure}

Un grafo dirigido, o digrafo, es un grafo en el que las aristas tienen direcciones. En un sentido más formal, un grafo dirigido es una tripleta $G = (V,E,\phi)$ donde $\phi$ es una función de incidencia que asigna cada arista a un par ordenado de nodos, es decir, $ \phi :E \to \{(x,y)\mid (x,y)\in V^{2}\ \mbox{ y } x \neq y \}$

 \begin{figure}[htbp]
   \centering
   \includesvg[width=0.25\textwidth]{figures/digraph.svg}
    \caption{Grafo Dirigido (\textit{DiGraph}). Podemos observar que la dirección de las aristas esta representada por una flecha que indica en donde se origina la arista (inicio de la flecha) .}
    \label{fig:digraph}
\end{figure}

Un subgrafo $H$ de un grafo $G$ es un grafo formado a partir de un subconjunto de nodos y un subconjunto de aristas de $G$. El subconjunto de nodos debe incluir todos los extremos del subconjunto de aristas, pero también puede incluir otros nodos. Un subgrafo inducido $H$ de un grafo $G$ es aquel que incluye todas las aristas del grafo $G$ cuyos puntos extremos pertenecen al subconjunto de nodos que define al subgrafo $H$.

Un isomorfismo de grafos es una biyección de los nodos de un grafo sobre otro, de modo que se preserva la adyacencia de los nodos. Formalmente, el isomorfismo entre dos grafos $G$ y $H$ es una función biyectiva  $f:V(G) \rightarrow V(H)$. Esta definición se extiende a las gráficas dirigidas si la función preserva el orden entre cada par de nodos asociados con una arista. 

Una simetría es un isomorfismo de una gráfica sobre sí misma.

Determinar si dos grafos con el mismo número de vértices $n$ y aristas $m$ son isomorfos o no, se conoce como el problema del isomorfismo de grafos. Este es un problema NP y se cree que es NP-Completo, aunque no está demostrado \cite{kobler_graph_1993}.

\label{subsection:isomorphism}

 \begin{figure}[htbp]
   \centering
   \includesvg[width=0.85\textwidth]{figures/isomorphism.svg}
    \caption{Ejemplo de Isomorfismo entre $G$ y $H$}
    \label{fig:isomorphism}
\end{figure}

 
%{Al uses the traditional brute-force method for determining graph isomorphism; enumerate and test all permutations of the vertices of Graph 2 until an isomorphism mapping is found. This determination is made by verifying that every edge in Graph 1 is present in Graph 2 using the currently enumerated permutation and vice versa. If all permutations are exhausted and an isomorphism has not been found, then the two graphs are not isomorphic. The time complexity of this 2 algorithm is O(N N!) where N is the number of vertices in graphs 1 and 2.}


\section{Aprendizaje automático}
\label{sec:ML}

El aprendizaje automático o aprendizaje de máquina, del inglés \textit{Machine Learning} (ML), es una rama de la Inteligencia Artificial (IA) que estudia algoritmos y técnicas que permiten automatizar soluciones a problemas complejos. Esto se logra a partir del aprendizaje sobre conjuntos de datos. 

Como subconjutnto de IA, un campo de estudio amplio y diverso que estudia distintas técnicas para crear algoritmos inteligentes, el aprendizaje automático se enfoca principalmente en imitar el aprendizaje humano y gradualmente mejorar la precisión sobre una tarea \cite{ibm_what_nodate}. En problemas complejos, a pesar de tener requerimientos claros y específicos, puede resultar complicado crear y programar conjuntos de reglas explícitas que representen una solución. Un ejemplo claro podría ser la tarea de detectar objetos en una imagen \cite{rebala_introduction_2019}.

Los algoritmos de aprendizaje automático son capaces de resolver problemas de manera {\it implícita}, aprendiendo estructuras y reglas a partir de un conjunto de datos en vez de tener una estructura y diseño explícito.  La naturaleza de estos algoritmos hace que dependan directamente de la calidad y cantidad de ejemplos en el conjunto de datos. Dependiendo de la naturaleza de la tarea planteada en el conjunto de datos, encontraremos distintas categorías de algoritmos dentro del aprendizaje automático \cite{rebala_introduction_2019}. Principalmente, se establece una distinción entre los conjuntos de datos etiquetas y no etiquetados.

Un conjunto de datos etiquetado es aquel cuyos ejemplos tienen la respuesta a la pregunta que se hace. Podemos pensar al conjunto de datos etiquetado como una guía de estudio, a partir de la cual el estudiante (en este caso la máquina) puede aprender de ejemplos. En el caso de la tarea de clasificación, el conjunto de datos contiene información sobre la clase representada por cada objeto; por ejemplo, una imagen de un perro contiene la etiqueta perro. 

Por otro lado, los datos no etiquetados son aquellos que no contienen una etiqueta, es decir que no han sido catalogados de ninguna manera y de los cuales no poseemos más información que el dato en sí. Los datos no etiquetados son, por ejemplo, aquellos que podemos recolectar de un sensor a partir de observaciones de algún entorno.

\subsection{Aprendizaje automático supervisado}
El objetivo del aprendizaje supervisado es crear un modelo sobre un conjunto de datos etiquetados para posteriormente predecir las etiquetas de datos nuevos a partir del aprendizaje de la relación entre las características y la variable objetivo \cite{rebala_introduction_2019}. En otras palabras, estos algoritmos resuelven problemas a partir de generar un modelo que aprende sobre un conjunto de datos con etiquetas conocidas (datos de entrenamiento) y que después se ejecuta sobre nuevos datos para predecir su etiqueta. 

Durante la fase de entrenamiento, el modelo ajusta sus parámetros para minimizar la diferencia entre las predicciones y los valores reales de la variable objetivo en el conjunto de datos de entrenamiento. Para lograr esto, el conjunto de datos etiquetados es dividido, una parte del conjunto se utiliza para que el algoritmo aprenda (como una guía con ejemplos) y a la vez otra parte más pequeña es utilizada para poner a prueba el entrenamiento (como un examen). Una vez que el modelo ha sido entrenado y se ha ajustado a los datos, este es capaz de etiquetar datos nuevos que no se habían visto previamente durante el entrenamiento. 

Este tipo de algoritmos tienden a ser muy efectivos. No obstante, una consideración importante es que no siempre es clara la manera en la que el problema esta siendo resuelto y por lo tanto es complicado interpretar el modelo \cite{rebala_introduction_2019}. 

Aunque se sabe qué entradas se deben proporcionar y qué salidas se deben esperar, no es sencillo comprender cómo el modelo llega a esas salidas; a este problema se le conoce como el problema de interpretabilidad de un modelo \cite{zhang_survey_2021, rebala_introduction_2019}. 

La interpretabilidad es un problema especialmente presente cuando se usa un enfoque de redes neuronales \cite{zhang_survey_2021}, que tienen millones de parámetros y vuelven complicado interpretar las decisiones o la serie de reglas que construyen para resolver un problema. Lo modelos recientes tienen una cantidad creciente de capas y conexiones que pueden procesar grandes cantidades de datos, y pueden aprender patrones muy sutiles que son difíciles de detectar por los humanos. 

La interpretabilidad puede ser un problema en algunas aplicaciones en donde es necesaria la transparencia. Un ejemplo podría ser el área médica, en donde es deseable que los algoritmos que asisten diagnósticos sean auditables y permitan revisar bajo qué criterios se realizó un diagnóstico.

\subsection{Aprendizaje automático no supervisado}

En el aprendizaje no supervisado el objetivo principal es aprender patrones a partir de conjuntos de datos no etiquetados. Dentro del aprendizaje automático no supervisado existen tareas como la identificación de patrones frecuentes, creación de reglas de asociación y búsqueda de agrupamientos \cite{kubat_introduction_2017}. En esta sección nos enfocaremos en la tarea de agrupar, que es quizá la tarea más representativa del aprendizaje no supervisado. 

Llevar a cabo una tarea de agrupamiento, o \textit{clustering}, consiste en dividir un gran conjunto de datos (puntos) de tal manera que los puntos con propiedades o patrones en común se encuentren en un mismo grupo. La complejidad de esta tarea radica en que los grupos no se conocen previamente y la cantidad de los mismos es desconocida. 

Los resultados de un agrupamiento pueden ser utilizados como clasificadores o predictores de valores de atributos desconocidos, e incluso como herramientas de visualización \cite{kubat_introduction_2017}.

 \begin{figure}[htbp]
   \centering
   \includesvg[width=0.6\textwidth]{figures/cluster-example.svg}
    \caption{Un ejemplo de agrupamiento (\texit{clustering}) en $R^2$.}
    \label{fig:clustering-example}
\end{figure}

Un ejemplo sencillo de agrupamiento en $R^2$ puede ser el que se muestra en la Fig. \ref{fig:clustering-example}, en el que cada punto representa un ejemplo descrito por dos atributos. Aunque en este caso es sencillo encontrar los agrupamientos a simple vista, para cuatro dimensiones o más no es posible visualizar los datos ni los grupos. A medida que los datos tienen mayor complejidad, establecer grupos sólo puede lograrse mediante algoritmos diseñados para la tarea \cite{kubat_introduction_2017}.

Los algoritmos de agrupamiento frecuentemente requieren definir una función de distancia entre un ejemplo y todos los elementos del grupo. Dependiendo de la naturaleza de los atributos, distintas medidas pueden ser convenientes. 

Una elección común es la distancia Minkowski. Consideremos $X = (x_1,x_2,\ldots,x_n)$ y $Y=(y_1,y_2,\ldots ,y_n) \in R^n$, la distancia Minkowski de orden $p$, con $p$ un número entero, se define como 
$$D(X,Y) = (\sum_{i=1}^{n}|x_{i}-y_{i}|^{p})^{\frac{1}{p}}.$$

Uno de los algoritmos más conocidos de agrupamiento es \textit{KMeans}. Este algoritmo agrupa los datos de entrada en $K$ grupos, para una $K$ predefinida por el usuario. La representación matemática de cada uno de los $K$ grupos es un centroide, que es el punto promedio de la distancia entre los puntos del grupo que representa. Por ser el valor promedio de todos los elementos del grupo, un centroide permite una caracterización adecuada del grupo. 

 \begin{figure}[htbp]
   \centering
   \includesvg[width=1\textwidth]{figures/centroids-example.svg}
    \caption{Centroides.}
    \label{fig:centroides}
\end{figure}

El algoritmo \textit{KMeans} busca minimizar la distancia promedio de cada punto al centroide del grupo al que fue asignado. De manera formal, dado un conjunto de ejemplos $(x_1, x_2, ..., x_n)$ donde cada ejemplo es un vector $d-dimensional$, \textit{KMeans} busca agrupar los $n$ ejemplos en $K(<=n)$ grupos $S = {S_1, S_2, ..., S_k}$ de tal manera que se minimice el error cuadrático total entre los ejemplos de entrenamiento y sus centroides correspondientes. Es decir, se busca resolver el problema 

\begin{align} \label{eq:kMeans}
\displaystyle {\underset {\mathbf {S} }{\operatorname {arg\,min} }}\sum _{i=1}^{k}\sum _{\mathbf {x} \in S_{i}}\left\|\mathbf {x} -{\boldsymbol {\mu }}_{i}\right\|^{2},
\end{align}
en donde $\mu_i$ representa el centroide del grupo $S_i$. 

La función objetivo en la expresión anterior se conoce como \textit{inertia} o \textit{within-cluster sum-of-squares criterion}.

El problema \ref{eq:kMeans} puede resolverse de manera iterativa, siguiendo el Algoritmo \ref{algorithms:k-means}. En este enfoque, los centroides pueden ser inicializados de manera aleatoria o con algunas técnicas de inicialización que permitan al algoritmo converger más rápido. Posterioremente, el algoritmo itera recalculando los centroides y los puntos correspondientes a cada grupo, hasta que ya no haya un cambio significativo en la función objetivo o se alcance el número máximo de iteraciones. 

Uno de los aspectos negativos de este algoritmo es que es muy susceptible a los centroides iniciales. En la práctica, es común ejecutar el algoritmo varias veces y considerar al mejor resultado, en términos de la función objetivo, como el agrupamiento final.

\include{codes/kmeans}\label{algorithms:k-means}

\section{Agrupamientos en grafos}
Algoritmos de agrupamiento como KMeans, diseñados para trabajar en conjuntos de vectores, no pueden ser utilizados directamente en grafos. Sin embargo, las numerosas aplicaciones y la necesidad en diferentes contextos de identificar grupos en datos estructurados han derivado en la propuesta de numerosos algoritmos para hacer agrupamientos en grafos.

Conviene señalar que al hablar de agrupamiento en grafos se abarcan dos tareas distintas. La primera consiste en la detección de comunidades, o grupos de nodos, dentro de un solo grafo. La segunda, significativamente menos explorada en la literatura, consiste en realizar agrupamientos a nivel grafo, es decir, identificar grupos de grafos con características en común dentro de una colección.  

A continuación se describen brevemente algunas características de estos dos problemas. 
\subsection{Agrupamientos de nodos}
\label{section:nodeclustering}
Realizar agrupamientos de nodos dentro de una red ha sido un problema ampliamente explorado en años recientes, debido a la gran cantidad de aplicaciones. Este problema se subdivide en dos tareas, conocidas como partición de grafos y detección de comunidades. Aunque ambas tareas se refieren a la división de los nodos de una red en grupos según el patrón de aristas de la red,  en la primera (partición) se conoce previamente el número de grupos que se busca, mientras que en la segunda (detección de comunidades), determinar el número de grupos es parte del problema \cite{newman_networks_2010}.

La partición de grafos es un problema estudiado desde 1960 \cite{newman_networks_2010}, y se enfoca en dividir los nodos de un grafo en $n$ grupos de tal manera que las aristas entre grupos sean las menores posibles. Al número de aristas entre cada grupo se le llama \emph{tamaño de corte (cut size)}. 

\begin{figure}[htbp]
   \centering
   \includesvg[width=0.75\textwidth]{figures/partition.svg}
    \caption{Nodos de una red divididos en 2 grupos donde el color del nodo representa el grupo al que pertenece.}
    \label{fig:partition}
\end{figure}

Por otro lado, la detección de comunidades busca grupos que ocurren naturalmente en la estructura de una red, independientemente de la cantidad de grupos y el número de nodos en ellos. Esta tarea nos permite estudiar la estructura y organización de una red. 

\subsection{Agrupamientos de grafos}
Encontrar grupos dentro de una colección de grafos es un problema menos estudiado que el agrupamiento de nodos. 

Comparar redes que podrían tener diferente tamaño y orden requiere basarse en propiedades estructurales. Este es un problema con aplicaciones de importancia, pero que puede ser computacionalmente costoso. 

 \begin{figure}[htbp]
   \centering
   \includesvg[width=0.75\textwidth]{figures/netcluster.svg}
    \caption{Dos agrupamientos de distintas redes de acuerdo a sus propiedades estructurales. En el primer grupo podemos observar {\it egonetworks}; en el segundo se muestran redes más complejas.}
    \label{fig:netcluster}
\end{figure}

En general, agrupar grafos requiere de dos herramientas: una función de distancia que nos permita comparar grafos entre sí, y un algoritmo de agrupamiento que haga uso de estas distancias para asignar cada grafo a un grupo determinado. 

Entre las aproximaciones populares para comparar dos grafos podemos encontrar el isomorfismo de grafo, la distancia de edición, el alineamiento de redes y la extracción de características.\cite{saxena_identifying_2019}. A continuación se describen dos enfoques populares para comparar grafos. 

%Idealmente el Isomorfismo de Grafo sería la aproximación más adecuada, no obstante como vimos en \ref{subsection:isomorphism} se trata de un problema NP y por lo tanto existen limitaciones mas que considerables en la práctica.

\paragraph{Medidas estructurales.} Es posible establecer una comparación entre dos grafos basada en propiedades de los mismos. Por ejemplo, el trabajo \textit{"Classifying Twitter Topic-Networks Using Social Network Analysis"} \cite{himelboim_classifying_2017}, relacionado muy de cerca a nuestra propuesta en cuanto a objetivo, utiliza las medidas de centralidad, centralización, densidad, modularidad y fracción de \textit{clusters} e \textit{Isolates} para establecer categorías de redes dentro de una colección, agrupando así conforme a características estructurales. Desafortunadamente este enfoque puede ser limitado en cuanto a la distinción que establece entre redes. Dependiendo de los indicadores considerados y del contexto del problema, el enfoque podría no capturar con suficiente detalle la estructura de las redes que se están comparando. 

\paragraph{Distancia de Edición (\textit{Graph Edit Distance}, GED).} Esta es una de las alternativas más conocidas para comparar dos grafos. La GED mide el número de cambios (inserciones y remociones de nodos y/o aristas) necesarios para llegar a la estructura del grafo $B$ partiendo desde el grafo $A$. 

\section{\textit{Representaciones vectoriales para grafos}}
Como se mencionó en la Sección \ref{sec:ML}, la mayoría de los algoritmos clásicos de aprendizaje automático no pueden ser utilizados directamente en redes, pues están diseñados para elementos de un espacio vectorial. Una de las estrategias recientes para resolver este problema es extraer características de los nodos o del grafo entero y utilizarlas para crear una representación vectorial. De esta manera es posible utilizar medidas clásicas de distancia en este espacio y aplicar algoritmos conocidos de aprendizaje de máquina. 

El proceso de extracción de características es llamado {\it representation learning}, y la representación vectorial se conoce como \textit{embedding}. En este documento utilizaremos estos dos términos de manera intercambiable.

El \textit{embedding} puede obtenerse para cada nodo o para representar un grafo entero. Existen numerosas técnicas para representar un nodo, las principales se basan en vecindario, rol estructural o atributos. En el caso del \textit{embedding} de un grafo, la extracción de características puede dividirse en dos principales categorías: técnicas basadas en la estructura global de la red y técnicas basadas en subestructuras de la red.

\subsection{\textit{Embeddings} a nivel de nodo}
Los \textit{embeddings} a nivel nodo han sido ampliamente explorados en años recientes, por ejemplo, grandes empresas tecnológicas los han utilizado para perfilar enormes cantidades de usuarios dentro de distintas redes sociales \cite{lerer_pytorch-biggraph_2019}. Otras aplicaciones también abarcan el área de biología y química \cite{yue_graph_2020}.

La idea principal en los \textit{embeddings} de nodos basados en vecindario es obtener el \textit{embedding} de cada nodo en función de los nodos vecinos y sus respectivos \textit{embeddings}. Una lista de algoritmos que siguen esta idea puede encontrarse en la Tabla \ref{table:neighNodeEmbedding}.

Por otro lado, los métodos basados en roles estructurales buscan generar un \textit{embedding} a partir de la extracción de características de los roles que toma cada nodo dentro de la red. Algoritmos que siguen este enfoque pueden encontrarse en la Tabla \ref{table:structuralNodeEmbedding}. 

\begin{longtable}
    \centering 
%\caption{Algunos ejemplos de algoritmos basados en el enfoque de embeddings de nodos basados en vecindarios}
\label{table:neighNodeEmbedding}
    \begin{tabular}{ |p{.6\textwidth}|p{.35\textwidth}|  }
    \hline
    \multicolumn{2}{|c|}{ \texit{Embeddings de nodos basados en el vecindario} } \\
    \hline
    Publicación & Algoritmo  \\
    \hline
    “Relational Learning via Latent Social Dimensions” \cite{tang_relational_2009} & SocioDim \\
    \hline
    “Billion-scale Network Embedding with Iterative Random Projection” \cite{zhang_billion-scale_2018} & RandNE \\
    \hline
    “GLEE: Geometric Laplacian Eigenmap Embedding” \cite{torres_glee_2020} & GLEE \\
    \hline
    “Diff2Vec: Fast Sequence Based Embedding with Diffusion Graphs” \cite{cornelius_fast_2018} & Diff2Vec \\
    \hline
    “NodeSketch: Highly-Efficient Graph Embeddings via Recursive Sketching” \cite{yang_nodesketch_2019} & NodeSketch \\
    \hline
    “Network Embedding as Matrix Factorization: Unifying DeepWalk LINE PTE and Node2Vec” \cite{qiu_network_2018} & NetMF   \\
    \hline
    “Multi-Level Network Embedding with Boosted Low-Rank Matrix Approximation” \cite{li_multi-level_2019} & BoostNE  \\
    \hline
    “Don’t Walk, Skip! Online Learning of Multi-scale Network Embeddings” \cite{perozzi_dont_2017} & Walklets  \\
    \hline
    “GraRep: Learning Graph Representations with Global Structural Information” \cite{cao_grarep_2015} & GraRep \\
    \hline
    “DeepWalk: Online Learning of Social Representations” \cite{perozzi_deepwalk_2014} & DeepWalk \\
    \hline
    “node2vec: Scalable Feature Learning for Networks” \cite{grover_node2vec_2016} & Node2Vec \\
    \hline
    “Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence” \cite{sun_alternating_2014} & NMFADMM \\
    \hline
    “Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering” \cite{dietterich_laplacian_2002} & LaplacianEigenmaps \\
    \hline
    \end{tabular}
\end{longtable}



\begin{table}
    \centering
\caption{Algunos ejemplos de algoritmos basados en el enfoque de embeddings de nodos basados en roles estructurales.}
\label{table:structuralNodeEmbedding}
    \begin{tabular}{ |p{8cm}|p{2cm}|  }
    \hline
    \multicolumn{2}{|c|}{Embedding estructural de nodos} \\
    \hline
    Publicación & Algoritmo  \\
    \hline
    “Learning Structural Node Embeddings Via Diffusion Wavelets” \cite{donnat_learning_2018} & GraphWave  \\
    \hline
    “Learning Role-based Graph Embeddings” \cite{ahmed_role-based_2022} & Role2Vec \\
    \hline
    \end{tabular}
\end{table}


\subsection{Embeddings a nivel de grafo}

Para construir representaciones vectoriales de un grafo es posible extraer características de la red completa o de estructuras locales dentro de ella.

En el primer caso, se utilizan propiedades enfocadas en la topología general de la red o características \emph{globales}. La Tabla \ref{table:graphEmbedding} muestra algunos ejemplos de algoritmos que utilizan este enfoque. 

En contraste, existen algoritmos que están enfocados en identificar y resumir estructuras locales dentro de la red, es decir, relaciones entre nodos. Los algoritmos que extraen características de los nodos examinan las estructuras locales haciendo uso de subgrafos, por ejemplo, \textit{EgoNetworks} o \graphlets. 

\begin{table}
\centering
\caption{}
\label{table:graphEmbedding}
    \begin{tabular}{ |p{8cm}|p{2cm}|  }
    \hline
    \multicolumn{2}{|c|}{Embedding de grafos} \\
    \hline
    Publicación & Algoritmo \\
    \hline
    “Graph2Vec: Learning Distributed Representations of Graphs” \cite{narayanan_graph2vec_2017} & Graph2Vec  \\
    \hline
    “A Simple Baseline Algorithm for Graph Classification” \cite{de_lara_simple_2018} & SF \\
    \hline
    “NetLSD: Hearing the Shape of a Graph” \cite{tsitsulin_netlsd_2018} & NetLSD \\
    \hline
    “GL2vec: Graph Embedding Enriched by Line Graphs with Edge Features” & GL2Vec \\
    \hline
    “Geometric Scattering for Graph Data Analysis” \cite{gao_geometric_nodate} & GeoScattering \\
    \hline
    “Invariant Embedding for Graph Classification” \cite{galland_invariant_nodate} & IGE \\
    \hline
    \end{tabular}
\end{table}

En general los algoritmos que hacen uso de \textit{embeddings} para agrupar redes siguen cuatro pasos principales que se describen a continuación.
\begin{itemize}
    \item Extracción de características: Se extraen patrones o características de la estructura topológica de los grafos a agrupar.
    
    \item Agregación de características: Se agregan estas características a los vectores que caracterizarán el grafo para de esta manera componer los \textit{embeddings} de los grafos.
    
    \item Cálculo de la distancia: Calcular la distancia entre los vectores de los grafos para cuantificar la similitud entre los mismos.
    
    \item Agrupar grafos: Agrupar los grafos más cercanos.
\end{itemize}

Recientemente se han adaptado las Redes Neuronales para trabajar con grafos. Algoritmos como Pytorch:BiGraph \cite{lerer_pytorch-biggraph_2019} son extremadamente eficientes a la hora de obtener \textit{embeddings} para nodos en redes grandes. No obstante al igual que otras algoritmos de esta clase heredan las problemáticas de interpretación de las redes neuronales.

El problema que se aborda en este trabajo tiene como principal objetivo explorar estructuras dentro de redes sociales, comprendiendo el papel que juegan diferentes usuarios. La interpretabilidad es clave en nuestro problema, pues no sólo queremos agrupar redes sino interpretar los motivos y entender qué características comparten redes que son agrupadas. Por esta razón, no usaremos {\it embeddings} generados con redes neuronales, sino basados en estructuras locales de la red. Específicamente, usaremos {\it graphlets}, que se detallan en el siguiente capítulo. 