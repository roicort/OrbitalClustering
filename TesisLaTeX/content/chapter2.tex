
\chapter{Agrupamiento sobre Redes} % Agrupamiento en redes 

%\cleanchapterquote{A picture is worth a thousand words. An interface is worth a thousand pictures.}{Ben Shneiderman}{(Professor for Computer Science)}

%\begin{lstlisting}[language=Java, caption={A simple Hellow World example in %Java.}\label{lst:javahelloworld}]
%public class HelloWorld {
%	public static void main ( String[] args ) {
%		// Output Hello World!
%		System.out.println( "Hello World!" );
%	}
%}
%\end{lstlisting}

En este capítulo se presentará el problema principal y los elementos necesarios para comprender la complejidad del mismo y una posible solución. Comenzaremos con una definición formal de un red así como su representación matemática, posteriormente se presenta el problema de agrupamiento y su relevancia dentro del aprendizaje automático. Finalmente este capítulo se enfoca en analizar las diferencias y retos de las tareas de agrupamiento en redes y agrupamiento sobre redes, así como de los diferentes enfoques y limitaciones para resolver estos problemas. Los elementos claves se presentan en esta sección sin embargo existe un apéndice que podría ayudar a resolver dudas adicionales.

\section{Redes}

Una red es un conjunto de nodos unidos por aristas que representan relaciones. Los nodos y aristas los podemos encontrar en distintas disciplinas con distintos nombres, por ejemplo en física se denominan sitios y vínculos y en sociología actores y vínculos. 

La representación matemática de un red se denomina grafo y es estudiada en matemáticas discretas y más específicamente en teoría de grafos. Un grafo esta formalmente definido como un par ordenado de conjuntos $G = (V,E)$, donde $V$ es un conjunto de nodos (vértices) y $E$ aristas (edges). \cite{saoub_graph_2021}

 \begin{figure}[htbp]
   \centering
   \includesvg[width=0.25\textwidth]{figures/graph.svg}
    \caption{Grafo no dirigido de tres nodos y tres aristas.}
    \label{fig:graph}
\end{figure}

Un grafo dirigido o digrafo es un grafo en el que las aristas tienen direcciones. En el sentido más formal un grafo dirigido es una tripleta $G = (V,E,\phi)$ donde $\phi$ es una función de incidencia que asigna cada arista a un par ordenado de nodos, es decir, $ \phi :E \to \{(x,y)\mid (x,y)\in V^{2}\ \textrm{and} x \neq y \}$

 \begin{figure}[htbp]
   \centering
   \includesvg[width=0.25\textwidth]{figures/digraph.svg}
    \caption{Grafo Dirigido (DiGraph). Podemos observar que la dirección de las aristas esta representada por una flecha que indica de donde se origina la arista (inicio de la flecha) }
    \label{fig:digraph}
\end{figure}

Un subgrafo $H$ de un grafo $G$ es otro grafo formado a partir de un subconjunto de nodos y un subconjunto de aristas de $G$. El subconjunto de nodos debe incluir todos los extremos del subconjunto de aristas, pero también puede incluir otros nodos. Un subgrafo inducido $H$ de un grafo $G$ es aquel que incluye todas las aristas del grafo $G$ cuyos puntos extremos pertenecen al subconjunto de nodos que define al subgrafo $H$

Un isomorfismo de grafos es una biyección de los nodos de un grafo sobre otro, de modo que se preserva la adyacencia de los nodos. Formalmente, el isomorfismo entre dos grafos $G$ y $H$ es una función biyectiva que se define de la siguiente manera $f:V(G) \rightarrow V(H)$.

\label{subsection:isomorphism}

 \begin{figure}[htbp]
   \centering
   \includesvg[width=0.85\textwidth]{figures/isomorphism.svg}
    \caption{Ejemplo de Isomorfismo entre $G$ y $H$}
    \label{fig:isomorphism}
\end{figure}

Determinar si dos grafos con el mismo número de vértices $n$ y aristas $m$ son isomorfos o no, se conoce como el problema del isomorfismo de grafos. Se considera a este problema un problema NP ya que no hay prueba de que sea NP-Completo. \cite{kobler_graph_1993} [Ver Apéndice] Por otro lado, se trata de un caso especial del problema de isomorfismo de subgrafos que sí se ha demostrado que es un problema NP-Completo. Resolver el problema de isomorfismo de grafos requeriría probar si las $n!$ biyecciones posibles preservan la adyacencia, sin embargo hasta ahora no se conoce un algoritmo general para resolver el problema y por lo tanto se considera un problema no resuelto dentro de la computación.

%{Al uses the traditional brute-force method for determining graph isomorphism; enumerate and test all permutations of the vertices of Graph 2 until an isomorphism mapping is found. This determination is made by verifying that every edge in Graph 1 is present in Graph 2 using the currently enumerated permutation and vice versa. If all permutations are exhausted and an isomorphism has not been found, then the two graphs are not isomorphic. The time complexity of this 2 algorithm is O(N N!) where N is the number of vertices in graphs 1 and 2.}


\section{Aprendizaje automático (\textit{Machine Learning})}
\label{sec:ML}

El Aprendizaje Automático o Aprendizaje de Máquinas, en inglés Machine Learning (ML), es una rama de la Inteligencia Artificial que estudia algoritmos y técnicas que permitan automatizar soluciones a problemas complejos a partir del aprendizaje sobre conjuntos de datos en vez de los métodos convencionales de programación. 

Dentro de la Inteligencia Artificial, que es un campo de estudio muy amplio y utiliza distintas técnicas para crear algoritmos inteligentes, el Aprendizaje Automático se enfoca principalmente en imitar el aprendizaje humano y gradualmente mejorar la precisión sobre una tarea \cite{ibm_what_nodate}. En la programación convencional dados ciertos requerimientos se diseña un programa que siga una serie de pasos para resolver un problema. No obstante en problemas complejos, a pesar de tener requerimientos claros y específicos este enfoque puede resultar complicado para crear y programar grandes conjuntos de reglas, pensemos por ejemplo en la tarea de detectar objetos en una imagen \cite{rebala_introduction_2019}.

Los algoritmos de Aprendizaje Automático son capaces de resolver problemas de una manera un tanto más genérica aprendiendo estructuras y reglas a partir de un conjunto de datos en vez de tener una estructura y diseño explicito. Por esta razón este tipo de algoritmos dependen directamente de la calidad y cantidad de ejemplos en el conjunto de datos. Estos ejemplos pueden tener etiquetas o ser datos crudos y dependiendo de la naturaleza del conjunto de datos encontraremos distintas categorías de algoritmos dentro del Aprendizaje Automático \cite{rebala_introduction_2019}. Un conjunto de datos etiquetado es aquel cuyos ejemplos tienen la respuesta a la pregunta que se hace. Podemos pensar al conjunto de datos etiquetado como una guía de estudio, a partir de la cual el estudiante (en este caso la máquina) puede aprender a partir de ejemplos. Un claro ejemplo es el caso de la tarea de clasificación, para la cual el conjunto de datos contiene información sobre la clase a la que representa cada objeto, por ejemplo, una imagen de un perro contiene la etiqueta perro. Por otro lado los datos no etiquetados (crudos) son aquellos que no contienen una etiqueta, es decir que no ha sido preprocesados de ninguna manera y de los cuales no poseemos más información que el dato en sí. Los datos no etiquetados son aquellos que podemos recolectar de un sensor a partir de observaciones de algún entorno.

\subsection{Aprendizaje Automático Supervisado}

El objetivo del Aprendizaje Supervisado es crear un modelo sobre un conjunto de datos etiquetados para posteriormente poder predecir las etiquetas de datos crudos \cite{rebala_introduction_2019}. La manera en la que estos algoritmos resuelven problemas es a partir de un modelo generado aprendiendo (con un entrenamiento) sobre un conjunto de datos con etiquetas conocidas (ejemplos) que después se ejecuta sobre nuevos datos para predecir su etiqueta. Durante la fase de entrenamiento el conjunto de datos etiquetados es dividido, una parte del conjunto se utiliza para que el algoritmo aprenda (cómo una guía con ejemplos) y a la vez otra parte más pequeña es utilizada para poner a prueba el entrenamiento (cómo un examen). Una vez que el entrenamiento se ha completado y el modelo se ha ajustado a los datos, este es capaz de etiquetar datos nuevos que no se habían visto previamente durante el entrenamiento. 

Estos algoritmos tienden a ser más efectivos que los modelos creados por humanos ya que pueden considerar mas atributos sobre un ejemplo y pueden procesar una cantidad superior de datos, no obstante una consideración importante es que no siempre es clara la manera en la que el problema esta siendo resuelto y por lo tanto es complicado interpretar el modelo \cite{rebala_introduction_2019}. 

\subsection{Aprendizaje Automático No-Supervisado}

En el Aprendizaje no Supervisado el objetivo principal es aprender patrones a partir de conjuntos de datos no etiquetados. Dentro del Aprendizaje Automático No-Supervisado existen tareas como la identificación de patrones frecuentes, creación reglas de asociación y búsqueda de agrupamientos \cite{kubat_introduction_2017}. En este capítulo nos enfocaremos en la tarea de agrupar.

\paragraph{Agrupamientos}

La tarea quizás mas representativa de Aprendizaje No-Supervisado es el \textit{Clustering} o Agrupamiento, se trata de dividir un gran conjunto de datos (puntos) de tal manera en que los puntos con propiedades o patrones en común se encuentren en un mismo grupo. La complejidad de esta tarea radica en que los grupos no se conocen previamente y la cantidad de los mismos es desconocida. Posteriormente lo resultados de esta tarea pueden ser utilizados como clasificadores o predictores de valores de atributos desconocidos, e incluso como herramientas de visualización. \cite{kubat_introduction_2017}

 \begin{figure}[htbp]
   \centering
   \includesvg[width=0.6\textwidth]{figures/cluster-example.svg}
    \caption{Un ejemplo de Clustering en $R^2$}
    \label{fig:clustering-example}
\end{figure}

Un ejemplo sencillo de agrupamiento en $R^2$ puede ser el de la Fig. \ref{fig:clustering-example}. Aquí cada punto representa un ejemplo descrito por dos atributos. En este caso es sencillo encontrar los agrupamientos a simple vista (ojímetro), sin embargo para cuatro dimensiones o más, no es posible para los humanos visualizar los datos ni los grupos; estos casos solo pueden ser resueltos por los algoritmos de agrupamiento. \cite{kubat_introduction_2017}

Los algoritmos de agrupamiento frecuentemente requieren definir una función de distancia entre un ejemplo y todos los elementos del grupo. Dependiendo de la naturaleza de los atributos distintas medidas pueden ser propuestas; cuando se trata de puntos numéricos en el espacio comúnmente se utiliza la distancia Minkowski. $X = (x_1,x_2,\ldots,x_n)$ y $Y=(y_1,y_2,\ldots ,y_n) \in R^n$
 
$$D(X,Y) = (\sum_{i=1}^{n}|x_{i}-y_{i}|^{p})^{\frac{1}{p}}$$

Quizás uno de los algoritmos más conocidos de Agrupamiento es \textit{K-Means}. Este algoritmo agrupa los datos de entrada en $K$ grupos para una $K$ predefinida por el usuario. Como cada ejemplo no incluye una etiqueta de la clase o grupo al que pertenece se trata inherentemente de un algoritmo de Aprendizaje No-Supervisado. La representación matemática de los $K$ grupos se conoce como\textit{centroide}, que es el punto promedio de la distancia a cada punto del grupo que representa. La interpretación de cada grupo, representado por su centroide, puede ser que su valor promedio es la caracterización de todos los elementos del grupo.

 \begin{figure}[htbp]
   \centering
   \includesvg[width=1\textwidth]{figures/centroids-example.svg}
    \caption{Centroides}
    \label{fig:centroides}
\end{figure}

Por lo tanto, el algoritmo de \textit{K-Means} busca minimzar la distancia promedio de cada centroide a los puntos de su grupo. De manera formal, el criterio de optimización es minimizar el error cuadrático total entre los ejemplos de entrenamiento y sus centroides correspondientes. La función objetivo se conoce como \textit{inertia} o \textit{within-cluster sum-of-squares criterion}.

$$\displaystyle {\underset {\mathbf {S} }{\operatorname {arg\,min} }}\sum _{i=1}^{k}\sum _{\mathbf {x} \in S_{i}}\left\|\mathbf {x} -{\boldsymbol {\mu }}_{i}\right\|^{2}$$

Dado un conjunto de ejemplos $(x_1, x_2, ..., x_n)$ donde cada ejemplo es un vector $d-dimensional$, K-Means busca agrupar los n ejemplos en $K(<=n)$ grupos $S = {S_1, S_2, ..., S_k}$ de tal manera que se minimice la suma de las distancias cuadradas para cada grupo.

Los centroides pueden ser inicializados de manera aleatoria o con algunas técnicas de inicialización que permitan al algoritmo converger más rápido. El algoritmo se itera recalculando los centroides y los puntos correspondientes hasta que ya no hay un cambio significativo o se ejecuta el número máximo de iteraciones. Uno de los aspectos negativos de este algoritmo es que es muy susceptible a las condiciones iniciales y por lo tanto se recomienda ejecutar el algoritmo varias veces para quedarse con el mejor resultado o promediarlos.

\lstinputlisting[language=Python, caption={Pseudocódigo $K-Means$ \cite{kubat_introduction_2017}}\label{algorithms:k-means}]{codes/kmeans.pseudo} 

\subsection{Agrupamientos y grafos}

A pesar de que existen numerosos algoritmos de agrupamiento, estos no pueden ser utilizados directamente en grafos. Debido a la representación y la dificultad de encontrar una función de distancia o similitud entre dos nodos o dos grafos, encontrar un agrupamiento no es un problema trivial. 

Debido a las aplicaciones y a la necesidad de extraer información de este tipo de estructuras de datos, recientemente se han propuesto numerosos algoritmos para hacer agrupamientos en grafos. La primera tarea que podemos encontrar dentro de estos algoritmos es la detección de grupos o comunidades dentro de la misma red. La segunda tarea que es un tanto más compleja y ha sido menos explorada es la de realizar agrupamientos a nivel grafo, es decir dentro de una colección de grafos agrupar aquellos que tengan características en común.

\section{Agrupamientos de Nodos}
\label{section:nodeclustering}

Realizar agrupamientos de nodos dentro de la red ha sido un problema ampliamente explorado en años recientes debido a la gran cantidad de aplicaciones. Este problema se subdivide en dos conocidos como partición de grafos y detección de comunidades, ambos problemas se refieren a la división de los nodos de una red en grupos, clusters o comunidades según el patrón de aristas de la red.\cite{newman_networks_2010} Podemos diferenciar uno del otro a partir de si conocemos previamente el número de grupos para la agrupación (partición) o es parte del problema y es desconocido (detección de comunidades).

La partición de grafos es un problema estudiado desde 1960 \cite{newman_networks_2010} y se trata de dividir los nodos de un grafo en $n$ grupos de tal manera que las aristas que entre ellos sean las menores posibles, a este número de aristas entre cada grupo se llama Tamaño de Corte (Cut Size). 

El caso más sencillo se llama Bisección en el que se divide la red en dos grupos y así recursivamente. A pesar de que es bastante sencillo de entender este problema, no es nada fácil de resolver. La idea mas intuitiva quizás podría ser analizar todas las particiones posibles de la red en dos grupos, calcular el tamaño del corte y quedarse con aquella que tenga el menor (fuerza bruta). No obstante esto es computacionalmente costoso para grafos muy grandes ya que las posibles particiones en dos grupos $n_1 y n_2$ para una red de $n$ nodos es de $\frac{n!}{n1!n2!}$ \cite{newman_networks_2010} por lo que encontrar la solución óptima es complicado, distintos algoritmos heurísticos que aproximen una solución óptima han sido ampliamente estudiados. Uno de los mas algoritmos heurísticos más sencillos para resolver la bisección de un grafo es el algoritmo \textit{Kernighan-Lin}.

 \begin{figure}[htbp]
   \centering
   \includesvg[width=0.75\textwidth]{figures/partition.svg}
    \caption{Nodos de una red divididos en 2 grupos donde el color del nodo representa el grupo al que pertenece.}
    \label{fig:partition}
\end{figure}

En el caso del problema de Detección de Comunidades, un reto adicional es el encontrar también el número y tamaño de grupos adecuado. La Detección de Comunidades busca grupos que ocurren naturalmente en la estructura de una red independientemente de la cantidad de grupos y el número de nodos en ellos. Estos algoritmos nos permiten descubrir y estudiar la estructura y organización de una red independientemente de su naturaleza.

\subsection{Agrupamientos de Grafos}

Como se mencionaba anteriormente, un problema aún más complejo, menos estudiado y por lo tanto, un reto más grande, es el de agrupar grafos completos. Comparar propiedades estructurales entre redes muy complejas es un problema importante con distintas aplicaciones científicas, sin embargo también es un problema computacionalmente costoso.

 \begin{figure}[htbp]
   \centering
   \includesvg[width=0.75\textwidth]{figures/netcluster.svg}
    \caption{}
    \label{fig:netcluster}
\end{figure}


En general agrupar dos grafos requiere de dos pasos, una función de distancia que nos permita comparar grafos entre sí y un algoritmo de agrupamiento que haga uso de estas distancias para asignar cada grafo a un grupo determinado. Dentro de las aproximaciones populares para comparar dos grafos podemos encontrar el Isomorfismo de Grafo, la Distancia de Edición, el Alineamiento de Redes y la Extracción de Características.\cite{saxena_identifying_2019}.
Idealmente el Isomorfismo de Grafo sería la aproximación más adecuada, no obstante como vimos en \ref{subsection:isomorphism} se trata de un problema NP y por lo tanto existen limitaciones mas que considerables en la práctica.

A pesar de que existen distintas medidas estructurales y de distancia, no suelen haber sido pensadas teniendo en mente la tarea de agrupamiento de grafos. Algunos ejemplos que podemos encontrar en la literatura al respecto son los siguientes.

\paragraph{Medidas Estructurales} 
Se han propuesto distintas medidas estructurales para capturar patrones dentro de una red. Este trabajo parte de hecho de las ideas propuestas en \textit{Classifying Twitter Topic-Networks Using Social Network Analysis} \cite{himelboim_classifying_2017} En este trabajo se utilizan medidas como la Centralidad, la Centralización, la Densidad, la Modularidad y la Fracción de \textit{Clusters} e \textit{Isolates} para clasificar redes enteras dependiendo de sus características estructurales. Aplicando esta metodología al conjunto de datos con el que se trabajó inicialmente nos dimos cuenta de que existían ciertas limitaciones respecto a la información que capturan estas medidas estructurales sobre la red.

\paragraph{GED} La Distancia de Edición (Graph Edit Distance, GED) puede ser una de las alternativas más conocidas para comparar dos grafos. La GED mide el número de cambios necesarios para llegar a la estructura del grafo $B$ partiendo desde el grafo $A$



\section{\textit{Representation Learning y Embeddings}}

Como discutíamos anteriormente los algoritmos clásicos de Aprendizaje de Máquina \ref{sec:ML} no puede ser utilizados directamente para realizar agrupamientos en redes debido a las dificultades anteriormente mencionadas. Una de las estrategias recientes para resolver este problema es extraer características de los nodos o el grafo entero y utilizarlas para crear de una representación vectorial y de esta manera poder utilizar medidas clásicas de distancia en este espacio y algoritmos clásicos de aprendizaje de máquina. Este proceso de extracción de características es llamado Representation Learning y a la representación de las mismas \textit{embedding}, pero para el fin de este documento utilizaremos estos dos términos de manera intercambiable.

El \textit{embedding} puede obtenerse para cada nodo o para representar un grafo entero. Existen numerosas técnicas para representar un nodo, puede ser a partir de basado en su vecindario, por su estructura topológica o sus atributos. En el caso de el \textit{embedding} de un grafo la extracción de características puede dividirse en dos principales categorías: técnicas basadas en la topología global de la red y técnicas basadas en subestructuras a nivel de los nodos de la red.

\subsection{\textit{Embedding} Nivel Nodo}

Los \textit{embedding} a nivel nodo han sido ampliamente explorados en años recientes, en gran medida debido a los retos que enfrentan las grandes tecnológicas para perfilar enormes cantidades de usuarios dentro de distintas redes  \cite{lerer_pytorch-biggraph_2019} y por muchas más aplicaciones, por ejemplo en el área de biología y química.
A continuación se presentan algunos conceptos útiles para comprender muchos algoritmos para este tipo de tarea.

\paragraph{Random Walks}
Una Caminata Aleatoria (Random Walk) es un proceso estocástico en el espacio matemático que describe una trayectoria de pasos aleatorios. Las Caminatas Aleatorias son utilizadas para analizar y simular procesos aleatorios así como para calcular correlaciones entre los objetos de estudio \cite{xia_random_2019}. En Grafos, las Caminatas Aleatorias permiten calcular la distancia entre nodos y extraer características de la topología. Cada paso en la trayectoria se da de acuerdo a cierta distribución de probabilidad, esta probabilidad de transición entre nodos es un factor relevante para la magnitud de su correlación. Es decir, mientras mas asociados se encuentran dos nodos, mayor es su probabilidad de transición. Uno de los algoritmos más famosos que hace uso de esta técnica es PageRank, que hace caminatas aleatorias dentro del grafo de páginas web para calcular la importancia de cada una de ellas.

\subsubsection{Neighbourhood-based Node Embedding}

Esta familia de algoritmos extrae características de la vecindad de un grafo para obtener una representación. Existen distintas técnicas para obtener un vector como\textit{embedding}, a continuación una breve reseña de algunos algoritmos y los métodos que utilizan.

\begin{center}
    \begin{tabular}{ |p{8cm}|p{2cm}|  }
    \hline
    \multicolumn{2}{|c|}{Neighbourhood-Based Node Embedding} \\
    \hline
    Paper & Algoritmo  \\
    \hline
    “Relational Learning via Latent Social Dimensions” & SocioDim \\
    \hline
    “Billion-scale Network Embedding with Iterative Random Projection” & RandNE \\
    \hline
    “GLEE: Geometric Laplacian Eigenmap Embedding” & GLEE \\
    \hline
    “Diff2Vec: Fast Sequence Based Embedding with Diffusion Graphs” & Diff2Vec \\
    \hline
    “NodeSketch: Highly-Efficient Graph Embeddings via Recursive Sketching” & NodeSketch \\
    \hline
    “Network Embedding as Matrix Factorization: Unifying DeepWalk LINE PTE and Node2Vec” & NetMF  \\
    \hline
    \end{tabular}
\end{center}

\begin{center}
    \begin{tabular}{ |p{8cm}|p{2cm}|  }
    \hline
    \multicolumn{2}{|c|}{Neighbourhood-Based Node Embedding} \\
    \hline
    “Multi-Level Network Embedding with Boosted Low-Rank Matrix Approximation” & BoostNE  \\
    \hline
    “Don’t Walk, Skip! Online Learning of Multi-scale Network Embeddings” & Walklets \\
    \hline
    “GraRep: Learning Graph Representations with Global Structural Information” & GraRep \\
    \hline
    “DeepWalk: Online Learning of Social Representations” & DeepWalk \\
    \hline
    “node2vec: Scalable Feature Learning for Networks” & Node2Vec \\
    \hline
    “Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence” & NMFADMM \\
    \hline
    “Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering” & LaplacianEigenmaps \\
    \hline
    \end{tabular}
\end{center}


\subsubsection{Structural Node Embedding}

\begin{comment}
    \begin{center}
    \begin{tabular}{ |p{8cm}|p{2cm}|  }
    \hline
    \multicolumn{2}{|c|}{Structural Node Embedding} \\
    \hline
    Paper & Algoritmo  \\
    \hline
    “Learning Structural Node Embeddings Via Diffusion Wavelets” & GraphWave  \\
    \hline
    “Learning Role-based Graph Embeddings” & Role2Vec \\
    \hline
    \end{tabular}
    \end{center}
\end{comment}


\subsection{Embedding Nivel Grafo}

En general los algoritmos que hacen uso de \textit{embeddings} para agrupar redes siguen cuatro pasos principales que se describen a continuación.

\begin{itemize}
    \item Extracción de características: Se extraen patrones o características de la estructura topológica de los grafos a agrupar.
    
    \item Agregación de características: Se agregan estas características a los vectores que caracterizarán el grafo para de esta manera componer los \textit{embeddings} de los grafos.
    
    \item Cálculo de la distancia: Calcular la distancia entre los vectores de los grafos para cuantificar la similitud entre los mismos.
    
    \item Agrupar grafos: Agrupar los grafos más cercanos.
\end{itemize}

De las características a extraer dentro de un grafo se pueden extraer características de la red completa o del estructuras locales dentro de ella.

\subsubsection{Características de la Red}
Este tipo de propiedades se centran en la topología general de la red y tratan de extraer características globales. Los algoritmos enfocados a este tipo de extracción de características buscan compilar y resumir estructuras importantes dentro de la red utilizando por ejemplo,

\begin{comment}
    \begin{center}
    \begin{tabular}{ |p{8cm}|p{2cm}|  }
    \hline
    \multicolumn{2}{|c|}{Graph Embedding} \\
    \hline
    Paper & Algoritmo \\
    \hline
    “Graph2Vec: Learning Distributed Representations of Graphs” & Graph2Vec  \\
    \hline
    “Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs” & FGSD \\
    \hline
    “A Simple Baseline Algorithm for Graph Classification” & SF \\
    \hline
    “NetLSD: Hearing the Shape of a Graph” & NetLSD \\
    \hline
    “GL2vec: Graph Embedding Enriched by Line Graphs with Edge Features” & GL2Vec \\
    \hline
    “Geometric Scattering for Graph Data Analysis” & GeoScattering \\
    \hline
    “Invariant Embedding for Graph Classification” & IGE \\
    \hline
    \end{tabular}
    \end{center}
\end{comment}

\subsubsection{Características de los Nodos} Este tipo de propiedades a diferencia de las propiedades globales de la red se centran en características locales entre los nodos que la conforman. Los algoritmos que extraen características de los nodos examinan las estructuras locales haciendo uso de subgrafos, por ejemplo, EgoNetworks o Graphlets.

\section{Interpretabilidad}

A pesar de que los algoritmos de Aprendizaje de Máquina funcionan muy bien, muchas veces es difícil conocer las reglas que el algoritmo ha creado en el modelo aprendido y por lo tanto no es posible comprender como es que un problema esta siendo resuelto, a este problema se le conoce como el problema de interpretabilidad de un modelo. \cite{zhang_survey_2021} \cite{rebala_introduction_2019} 

Este problema esta especialmente presente en las Redes Neuronales que tienen millones de parámetros y por lo tanto es complicado interpretar las decisiones o la serie de reglas que llevan a cabo para resolver un problema. En algunas áreas es igual de importante la interpretabilidad del modelo que su precisión, un ejemplo claro es el de la medicina en donde los médicos deben ser capaces de interpretar y confirmar los resultados del diagnóstico de un algoritmo. En el contexto de Twitter es igualmente importante interpretar los motivos por los que las redes son agrupadas.

Recientemente se han adaptado las Redes Neuronales para trabajar con grafos. Algoritmos como Pytorch:BiGraph \cite{lerer_pytorch-biggraph_2019} son extremadamente eficientes a la hora de obtener \textit{embeddings} para nodos en redes enormes. No obstante al igual que otras algoritmos de esta clase heredan las problemáticas de interpretación de las redes neuronales.

%Explicar que en el contexto de Twitter nos importa no sólo cómo quedan los grupos sino por qué. Redes sociales, lo que sociólogos han hecho con estas ideas (como en la parte de antecedentes del paper).